# ğŸ”§ LLM Training Puzzles: Pipeline + FSDP Simulation

This repository explores a minimalist and educational simulation of **Large Language Model (LLM)** training using **Pipeline Parallelism** and **Fully Sharded Data Parallel (FSDP)**. Designed as a learning tool, it breaks down complex training infrastructure into simple distributed async functions, making it ideal for studying model-parallel techniques in LLM training. The puzzles were originally developed by srush(https://github.com/srush).

---

## ğŸ“† Features

- ğŸš€ Simulated **Pipeline Parallelism** across model layers
- ğŸ§¹ Lightweight **Fully Sharded Data Parallelism (FSDP)** across weight shards
- ğŸ”€ Gradient accumulation across multiple microbatches
- ğŸ“¡ Simulated asynchronous communication (`pass_to`, `receive`, `allgather`)
- ğŸ§  Layer and rank-aware compute distribution
- âœ… Optimizer update step after backpropagation
- ğŸ§ª Torch-free implementation â€” great for algorithm visualization
- ğŸ“† Rank decomposition across shards and layers
- âš–ï¸ Model-parallel and data-parallel hybrid execution
- ğŸ”œ Manual forward/backward pass orchestration
- ğŸ”¢ Weight sharding and merging with `allgather`
- ğŸ“Š Basic loss and gradient computation + update step
- ğŸ”„ Manual microbatch-based gradient accumulation
- ğŸŒ Pipeline communication across pipeline stages
- ğŸ›‹ï¸ Memory optimization via sharding and delayed allgather
- ğŸ’ª Simulation of FSDP-style allgather and shard logic
- â³ Asynchronous scheduling and communication of activations and gradients


---

## ğŸ›  Core Concepts

### âœ… `pipeline_fsdp(model: Model)`

The main async training loop simulates:

1. **Weight Shard Loading**:

   - Each rank loads its shard of weights using `load_weights`.

2. **Forward Pass**:

   - Activations are computed per layer and passed downstream using `pass_to`.

3. **Backward Pass**:

   - Gradients are computed and passed upstream.
   - Final layers compute loss; intermediate layers receive activations and pass back gradients.

4. **Gradient Accumulation**:

   - Gradients are accumulated across microbatches for each layer owned by the rank.

5. **Optimizer Update**:

   - Final accumulated gradients are used in `model.update()` to update weights.

---

## ğŸ¤© Techniques Implemented

- Layer and shard-based rank decomposition
- Forward pipeline execution using simulated async messaging
- Asynchronous activation passing between pipeline stages
- Simulated allgather to reconstruct full weights from shards (FSDP-style)
- Backward pipeline with upstream gradient communication
- Microbatch-based gradient accumulation and update
- Loss computation on final layer outputs
- Optimizer update using local gradient and state
- Layer-local activation and gradient storage with cleanup
- Shard-local optimizer state tracking and weight updates
- Fake gradients for unused layers to ensure completeness
- Hybrid parallelism combining pipeline and sharding
- Manual implementation of pipeline communication graph
- Modular layer-wise update hooks

---

## ğŸ“œ Educational Value

This repo is excellent for:

- Researchers prototyping custom parallelism strategies
- Engineers learning FSDP or pipeline parallel logic
- Educators explaining the internals of LLM training

---


